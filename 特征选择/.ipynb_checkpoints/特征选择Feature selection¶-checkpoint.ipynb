{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该sklearn.feature_selection模块中的类可用于样本集的特征选择/降维，以提高估计量的准确性得分或提高其在超高维数据集上的性能。\n",
    "\n",
    "链接：\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 删除低方差的特征\n",
    "\n",
    "**VarianceThreshold**是特征选择的简单基准方法。它将删除方差未达到某个阈值的所有特征。默认情况下，它将删除所有零方差特征，即在所有样本中具有相同值的特征。\n",
    "\n",
    "这种特征选择算法只考虑特征(x) ，而不考虑期望的输出(y) ，因此可以用于___非监督式学习___\n",
    "\n",
    "官方说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold.get_support\n",
    "\n",
    "例如，假设我们有一个具有布尔特性的数据集，我们希望删除超过80% 的样本中的所有一或零(开或关)的特性。 布尔特征是贝努利随机变量，这些变量的方差由\n",
    "\n",
    "$Var(X)=p(1-p)$\n",
    "\n",
    "所以我们可以选择使用阈值:8 * (1-. 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "#sel.fit(X)+sel.transform(X)=sel.fit_transform(X)\n",
    "sel.fit(X)#从 x 中学习经验方差\n",
    "print(sel.transform(X))# 减少 x 到选定的特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "************************************************************\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [0 1 0]\n",
      " [0 1 1]]\n",
      "************************************************************\n",
      "{'threshold': 0.15999999999999998}\n",
      "************************************************************\n",
      "[False  True  True]\n"
     ]
    }
   ],
   "source": [
    "X_trans = sel.fit_transform(X)#学习数据，然后转换它\n",
    "print(X_trans)\n",
    "print('*'*60)\n",
    "print(sel.inverse_transform(X_trans))#逆变换\n",
    "print('*'*60)\n",
    "print(sel.get_params())#得到此估计量的参数\n",
    "print('*'*60)\n",
    "print(sel.get_support())#获取所选特性的整数索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如预期的那样，**VarianceThreshold** 删除了第一列，其中包含零的概率为 p=5 / 6 > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variances is [ 0.1875 13.6875 13.6875 13.6875]\n",
      "After transform is [[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [11 12 13]]\n",
      "The surport is [1 2 3]\n",
      "After reverse transform is [[ 0  1  2  3]\n",
      " [ 0  4  5  6]\n",
      " [ 0  7  8  9]\n",
      " [ 0 11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import  VarianceThreshold\n",
    "\n",
    "#数据预处理过滤式特征选取VarianceThreshold模型\n",
    "def test_VarianceThreshold():\n",
    "    X=[[100,1,2,3],\n",
    "       [100,4,5,6],\n",
    "       [100,7,8,9],\n",
    "       [101,11,12,13]]\n",
    "    selector=VarianceThreshold(1)#删除方差未达到1的所有特征\n",
    "    selector.fit(X)\n",
    "    print(\"Variances is %s\"%selector.variances_)#四个特征的方差\n",
    "    print(\"After transform is %s\"%selector.transform(X))#删除方差未达到1的特征，即第0列的特征\n",
    "    print(\"The surport is %s\"%selector.get_support(True))\n",
    "    print(\"After reverse transform is %s\"%selector.inverse_transform(selector.transform(X)))\n",
    "    \n",
    "#调用test_VarianceThreshold()\n",
    "test_VarianceThreshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单变量特征选择\n",
    "\n",
    "单变量特征选择通过基于单变量统计检验选择最佳特征来工作。可以将其视为估算器的预处理步骤。Scikit-learn将要素选择例程公开为实现该transform方法的对象："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SelectKBest** 除了 k 最高得分的功能外，SelectKBest 删除了所有功能\n",
    "官网说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "\n",
    "参数说明：class sklearn.feature_selection.SelectKBest(__score_func__=<function f_classif>, *, __k__=10)\n",
    "    \n",
    "___score_func__: 可调用函数。\n",
    "函数接受两个数组 x 和 y，并返回一对数组(scores，pvalue)或带 scores 的单个数组。 默认值是 f classif (参见下面的“另见”)。 默认函数只适用于分类任务。\n",
    "    \n",
    "官方提供的可用函数：\n",
    "- 分类： f_classif（分类任务中标签 / 特征之间的方差分析 f 值），mutual_info_classif（离散目标的互信息），chi2（用于分类任务的非负特性的卡方统计），\n",
    "    \n",
    "- 回归：f_regression（用于回归任务的标签 / 特征之间的 f 值），mutual_info_regression（连续目标的互信息），\n",
    "    \n",
    "    SelectPercentile（根据最高分的百分比选择特征）,SelectFpr（根据假阳性率测试选择特征），SelectFdr（根据估计的错误发现率选择特征），SelectFwe（基于分类错误率选择特性），GenericUnivariateSelect（可配置模式的单变量特征选择器）\n",
    "    \n",
    "__k__: 整数int或者'all'，默认为10。要选择的顶部特性的数量。“全部”选项绕过选择，用于参数搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile,mutual_info_classif\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.  0.4 1.6 3.6]\n",
      "[0.52708926 1.         0.52708926 0.20590321 0.05777957]\n"
     ]
    }
   ],
   "source": [
    "#利用chi2卡方分布计算每一个特征的卡方统计量和每一个特征的pval值\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "X=[[1,2,3,4,5],\n",
    "      [5,4,3,2,1],\n",
    "      [3,3,3,3,3,],\n",
    "      [1,1,1,1,1]]\n",
    "y=[0,1,0,1]\n",
    "scores_,pvalues_ = chi2(X,y)\n",
    "print(scores_)#每一个特征的卡方统计量\n",
    "print(pvalues_)#每一个特征的pval值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before transform: [[1, 2, 3, 4, 5], [5, 4, 3, 2, 1], [3, 3, 3, 3, 3], [1, 1, 1, 1, 1]]\n",
      "scores_: [0.4 0.  0.4 1.6 3.6]\n",
      "pvalues_: [0.52708926 1.         0.52708926 0.20590321 0.05777957]\n",
      "selected index: [2 3 4]\n",
      "after transform: [[3 4 5]\n",
      " [3 2 1]\n",
      " [3 3 3]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "#数据预处理过滤式特征选取SelectKBest模型\n",
    "def test_SelectKBest():\n",
    "    X=[[1,2,3,4,5],\n",
    "          [5,4,3,2,1],\n",
    "          [3,3,3,3,3,],\n",
    "          [1,1,1,1,1]]\n",
    "    y=[0,1,0,1]\n",
    "    print(\"before transform:\",X)\n",
    "    selector=SelectKBest(score_func=chi2,k=3)\n",
    "    selector.fit(X,y)\n",
    "    print(\"scores_:\",selector.scores_))#每一个特征的卡方统计量\n",
    "    print(\"pvalues_:\",selector.pvalues_)#每一个特征的pval值\n",
    "    print(\"selected index:\",selector.get_support(True))#选择卡方值前三的特征\n",
    "    print(\"after transform:\",selector.transform(X))\n",
    "    \n",
    "#调用test_SelectKBest()\n",
    "test_SelectKBest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SelectPercentile** 删除除用户指定的最高得分百分比以外的所有特征\n",
    "\n",
    "官网说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile\n",
    "\n",
    "参数说明：class sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>, *, percentile=10)\n",
    "    \n",
    "___score_func__: 可调用函数。\n",
    "函数接受两个数组 x 和 y，并返回一对数组(scores，pvalue)或带 scores 的单个数组。 默认值是 f classif (参见下面的“另见”)。 默认函数只适用于分类任务。\n",
    "    \n",
    "官方提供的可用函数：\n",
    "- 分类： f_classif（分类任务中标签 / 特征之间的方差分析 f 值），mutual_info_classif（离散目标的互信息），chi2（用于分类任务的非负特性的卡方统计），\n",
    "    \n",
    "- 回归：f_regression（用于回归任务的标签 / 特征之间的 f 值），mutual_info_regression（连续目标的互信息），\n",
    "    \n",
    "    SelectKBest（根据 k 最高分选择特性）,SelectFpr（根据假阳性率测试选择特征），SelectFdr（根据估计的错误发现率选择特征），SelectFwe（基于分类错误率选择特性），GenericUnivariateSelect（可配置模式的单变量特征选择器）\n",
    "    \n",
    "__percentile__: 百分位数，默认为10。要保留的特性百分比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对每个特征使用通用的单变量统计检验：false positive rate **SelectFpr**, false discovery rate **SelectFdr**, or family wise error **SelectFwe**\n",
    "对每个特征使用常见的单变量统计检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFpr\n",
    "\n",
    "官网说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr\n",
    "\n",
    "参数说明：class sklearn.feature_selection.SelectFpr(score_func=<function f_classif>, *, alpha=0.05)\n",
    "\n",
    "___score_func__: 可调用函数。\n",
    "函数接受两个数组 x 和 y，并返回一对数组(scores，pvalue)或带 scores 的单个数组。 默认值是 f classif (参见下面的“另见”)。 默认函数只适用于分类任务。\n",
    "    \n",
    "官方提供的可用函数：\n",
    "- 分类： f_classif（分类任务中标签 / 特征之间的方差分析 f 值），mutual_info_classif（离散目标的互信息），chi2（用于分类任务的非负特性的卡方统计），\n",
    "    \n",
    "- 回归：f_regression（用于回归任务的标签 / 特征之间的 f 值），mutual_info_regression（连续目标的互信息），\n",
    "\n",
    "    \n",
    "__alpha__: 浮点型，默认为0.05。要保留的特性的最高 p 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectFpr, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n",
    "print(X_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFdr\n",
    "\n",
    "官网说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr\n",
    "\n",
    "参数说明：class sklearn.feature_selection.SelectFdr(score_func=<function f_classif>, *, alpha=0.05)\n",
    "\n",
    "___score_func__: 可调用函数。\n",
    "函数接受两个数组 x 和 y，并返回一对数组(scores，pvalue)或带 scores 的单个数组。 默认值是 f classif (参见下面的“另见”)。 默认函数只适用于分类任务。\n",
    "    \n",
    "官方提供的可用函数：\n",
    "- 分类： f_classif（分类任务中标签 / 特征之间的方差分析 f 值），mutual_info_classif（离散目标的互信息），chi2（用于分类任务的非负特性的卡方统计），\n",
    "    \n",
    "- 回归：f_regression（用于回归任务的标签 / 特征之间的 f 值），mutual_info_regression（连续目标的互信息），\n",
    "\n",
    "    \n",
    "__alpha__: 浮点型，默认为0.05。要保留的特性的最高 p 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectFdr, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFwe\n",
    "\n",
    "官网说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe\n",
    "\n",
    "参数说明：class sklearn.feature_selection.SelectFwe(score_func=<function f_classif>, *, alpha=0.05)\n",
    "\n",
    "___score_func__: 可调用函数。\n",
    "函数接受两个数组 x 和 y，并返回一对数组(scores，pvalue)或带 scores 的单个数组。 默认值是 f classif (参见下面的“另见”)。 默认函数只适用于分类任务。\n",
    "    \n",
    "官方提供的可用函数：\n",
    "- 分类： f_classif（分类任务中标签 / 特征之间的方差分析 f 值）,chi2（用于分类任务的非负特性的卡方统计），\n",
    "    \n",
    "- 回归：f_regression（用于回归任务的标签 / 特征之间的 f 值）\n",
    "\n",
    "    \n",
    "__alpha__: 浮点型，默认为0.05。要保留的特性的最高 p 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n",
    "print(X_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GenericUnivariateSelect**允许使用可配置策略执行单变量特征选择。 这允许选择最佳的单变量选择策略与超参数搜索估计。\n",
    "\n",
    "官网说明：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect\n",
    "\n",
    "参数说明：class sklearn.feature_selection.GenericUnivariateSelect(score_func=<function f_classif>, *, mode='percentile', param=1e-05)\n",
    "\n",
    "___score_func__: 可调用函数。\n",
    "函数接受两个数组 x 和 y，并返回一对数组(scores，pvalue)。 对于“百分位”或“最佳”模式，它可以返回单个数组得分。\n",
    "    \n",
    "__mode__: {‘percentile’, ‘k_best’, ‘fpr’, ‘fdr’, ‘fwe’}。\n",
    "特征选择模式\n",
    "    \n",
    "__param__: 浮点数或整型数取决于特征选择模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这个函数相当于第2部分单变量特征选择的函数集合，在这一个里可以实现上述5种单变量特征选择的方法，接下来就让我们测试一下结果是不是一致**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 20)\n"
     ]
    }
   ],
   "source": [
    "#对比SelectKBest函数，返现结果一致\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 7)\n"
     ]
    }
   ],
   "source": [
    "#对比SelectPercentile函数，返现结果一致\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, mode='percentile', param=10)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 16)\n"
     ]
    }
   ],
   "source": [
    "#对比SelectFpr函数，返现结果一致\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, mode='fpr', param=0.01)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 16)\n"
     ]
    }
   ],
   "source": [
    "#对比SelectFdr函数，返现结果一致\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, mode='fdr', param=0.01)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 15)\n"
     ]
    }
   ],
   "source": [
    "#对比SelectFwe函数，返现结果一致\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, mode='fwe', param=0.01)\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些对象以一个计分函数作为输入，该函数返回单变量得分和 p 值(或者只返回 SelectKBest 和 SelectPercentile 的得分) :\n",
    "\n",
    "- 分类： f_classif（分类任务中标签 / 特征之间的方差分析 f 值），mutual_info_classif（离散目标的互信息），chi2（用于分类任务的非负特性的卡方统计），\n",
    "- 回归：f_regression（用于回归任务的标签 / 特征之间的 f 值），mutual_info_regression（连续目标的互信息），\n",
    "\n",
    "基于 f 检验的方法估计了两个随机变量之间的线性相关度。 另一方面，互信息方法可以捕捉任何类型的统计相关性，但是作为非参数方法，它们需要更多的样本进行精确估计。\n",
    "\n",
    "**如果您使用稀疏数据（即数据表示为稀疏矩阵） chi2，mutual_info_regression，mutual_info_classif 会处理数据而不会使其变得密集。**\n",
    "\n",
    "**Warning警告** \n",
    "注意不要在分类问题上使用回归评分函数，否则会得到无用的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归特征消除\n",
    "\n",
    "给定一个将权重分配给特征（例如线性模型的系数）的外部估计器，递归特征消除（RFE）是通过递归考虑越来越少的特征集来选择特征。首先，对估计器进行初始特征集训练，并通过coef_属性或属性获得每个特征的重要性feature_importances_。然后，从当前的一组特征中删除最不重要的特征，然后对该过程进行递归重复，直到最终达到所需的特征数量。\n",
    "\n",
    "RFECV 在交叉验证循环中执行RFE，以找到最佳数量的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用SelectFromModel特征选择\n",
    "SelectFromModel是一个元变压器，可与拟合后具有coef_或feature_importances_属性的任何估计量一起使用。如果相应的coef_或feature_importances_值低于提供的 threshold参数，则认为这些功能不重要并已删除 。除了通过数字指定阈值之外，还内置了使用字符串参数查找阈值的试探法。可用的试探法是“均值”，“中位数”和浮点数的倍数，例如“ 0.1 *均值”。与threshold标准结合使用时，可以使用 max_features参数设置要选择的要素数量的限制。\n",
    "\n",
    "## 基于L1-特征选择\n",
    "受到L1范数惩罚的线性模型的解决方案稀疏：许多估计系数为零。当目标是减少要与另一个分类器一起使用的数据的维数时，可以将它们与feature_selection.SelectFromModel 选择非零系数一起使用。特别地，用于此目的的稀疏估计量是linear_model.Lasso用于回归，和linear_model.LogisticRegression以及svm.LinearSVC 用于分类的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于树的特征选择\n",
    "基于树的估计器（请参阅sklearn.tree模块和模块中的树林sklearn.ensemble）可用于计算基于杂质的特征重要性，而反过来又可用于丢弃不相关的特征（与sklearn.feature_selection.SelectFromModel meta-transformer 结合使用）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[0.06409045 0.06899865 0.44150118 0.42540972]\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "print(clf.feature_importances_ ) \n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 功能选择作为管道的一部分\n",
    "在进行实际学习之前，通常将特征选择用作预处理步骤。推荐在scikit-learn中执行此操作的方法是使用sklearn.pipeline.Pipeline：\n",
    "\n",
    "在此代码段中，我们sklearn.svm.LinearSVC 结合使用sklearn.feature_selection.SelectFromModel 来评估功能的重要性并选择最相关的功能。然后，sklearn.ensemble.RandomForestClassifier对转换后的输出（即仅使用相关特征）进行训练。您可以使用其他功能选择方法以及分类器执行类似的操作，这些分类器提供了一种评估功能的重要性的方法。有关sklearn.pipeline.Pipeline更多详细信息，请参见示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipe.fit(X_train, y_train)\n",
    "Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
